{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Preprocessor import Preprocessor\n",
    "from OfflineEvaluator import OfflineEvaluator\n",
    "from ContentAnalyzer import ContentAnalyzer\n",
    "from PostFilter import PostFilter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import pickle\n",
    "from STOPWORDS import STOP_WORDS\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from collections import Counter\n",
    "from itertools import combinations\n",
    "import utils\n",
    "from datetime import datetime\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample course SA\n",
    "dbnum = ##\n",
    "num_weeks = 6\n",
    "START_DATE = #datetime(...)\n",
    "TEMPORAL_START_WEEK = 3\n",
    "instructors = #[...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10\n",
    "week_stride = 1\n",
    "path_clean = \"cleaned_data\"\n",
    "path_sample = \"sample\"\n",
    "\n",
    "# preprocess\n",
    "WEIGHT_POST_LOOKUP = {1: 'created',2:'liked',3:'linked',4:'replied',5:'revisited',6:'read',7:'anonymously read'}\n",
    "WEIGHT_USER_LOOKUP = {1:'liked',2:'linked',3:'replied',4:'read',5:'anonymously read'}\n",
    "CUTOFF_WEIGHT = 5\n",
    "EXPLICIT_THRESHOLD = 5\n",
    "IMPLICIT_THRESHOLD = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhaorui/anaconda3/envs/main/lib/python3.7/site-packages/pandas/core/indexing.py:1418: FutureWarning: \n",
      "Passing list-likes to .loc or [] with any missing label will raise\n",
      "KeyError in the future, you can use .reindex() as an alternative.\n",
      "\n",
      "See the documentation here:\n",
      "https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#deprecate-loc-reindex-listlike\n",
      "  return self._getitem_tuple(key)\n"
     ]
    }
   ],
   "source": [
    "preprocessor = Preprocessor(path_sample, dbnum, \n",
    "                            WEIGHT_POST_LOOKUP, WEIGHT_USER_LOOKUP, START_DATE, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessor.preview_interaction_distribution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Universal Sentence Encoder embeddings and similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Item similarities\n",
    "precomputed_item_similarities = {}\n",
    "feature_size = 128 # for USE\n",
    "\n",
    "# IF USE TFIDF\n",
    "#precomputed_item_similarities = self.preprocessor.precompute_similarity('', 'tfidf+lsi', 64)\n",
    "\n",
    "# if use USE\n",
    "precomputed_item_similarities, sentence_embs = preprocessor.precompute_similarity('lib/use_pretrained_models/1', \n",
    "                                                                                  'sentence_emb', feature_size)\n",
    "# if use BERT\n",
    "#precomputed_item_similarities = self.preprocessor.precompute_similarity('cleaned_data/ft_model_incr', 'bert_word_emd+sif', 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cleaned_data/db%d/precomputed_item_similarities_use3.pickle'%dbnum, 'wb') as handle:\n",
    "    pickle.dump(precomputed_item_similarities, handle)\n",
    "with open('cleaned_data/db%d/precomputed_sen_embs.pickle'%dbnum, 'wb') as handle:\n",
    "    pickle.dump(sentence_embs, handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Content graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhaorui/anaconda3/envs/main/lib/python3.7/site-packages/pandas/core/indexing.py:1418: FutureWarning: \n",
      "Passing list-likes to .loc or [] with any missing label will raise\n",
      "KeyError in the future, you can use .reindex() as an alternative.\n",
      "\n",
      "See the documentation here:\n",
      "https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#deprecate-loc-reindex-listlike\n",
      "  return self._getitem_tuple(key)\n"
     ]
    }
   ],
   "source": [
    "evaluator = OfflineEvaluator(WEIGHT_POST_LOOKUP, WEIGHT_USER_LOOKUP, CUTOFF_WEIGHT, \n",
    "                             EXPLICIT_THRESHOLD, IMPLICIT_THRESHOLD, START_DATE, verbose=0)\n",
    "evaluator.set_ground_truth_criterion('explicit_implicit')\n",
    "evaluator.preprocess(path_sample, dbnum, num_weeks, week_stride)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_nums = 10\n",
    "ca = ContentAnalyzer(evaluator.all_note_contents)\n",
    "ca.preprocess(['NOUN','VERB','PROPN'])\n",
    "ca.extract_keywords(keyword_nums)\n",
    "graph = ca.construct_hypernym_graph()\n",
    "#print(nx.info(graph))\n",
    "nodes_hyp = [x for x,y in graph.nodes(data=True) if y['node_type']=='hypernym']\n",
    "nodes_hyp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.write_graphml_lxml(graph, \"cleaned_data/db%d/content_graph-%d.graphml\"%(dbnum,keyword_nums))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_nums = 5\n",
    "ca = ContentAnalyzer(evaluator.all_note_contents)\n",
    "ca.preprocess(['NOUN','VERB','PROPN'])\n",
    "ca.extract_keywords(keyword_nums)\n",
    "graph = ca.construct_hypernym_graph()\n",
    "#print(nx.info(graph))\n",
    "nodes_hyp = [x for x,y in graph.nodes(data=True) if y['node_type']=='hypernym']\n",
    "nodes_hyp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.write_graphml_lxml(graph, \"cleaned_data/db%d/content_graph-%d.graphml\"%(dbnum,keyword_nums))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_nums = 0.2\n",
    "ca = ContentAnalyzer(evaluator.all_note_contents)\n",
    "ca.preprocess(['NOUN','VERB','PROPN'])\n",
    "ca.extract_keywords(keyword_nums)\n",
    "graph = ca.construct_hypernym_graph()\n",
    "#print(nx.info(graph))\n",
    "nodes_hyp = [x for x,y in graph.nodes(data=True) if y['node_type']=='hypernym']\n",
    "nodes_hyp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.write_graphml_lxml(graph, \"cleaned_data/db%d/content_graph-%.1f.graphml\"%(dbnum,keyword_nums))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post filter and Post Creation Dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Calculate post creation dates\n",
    "'''\n",
    "\n",
    "lst = []\n",
    "post_create_dates_all = {}\n",
    "ct = 0\n",
    "for u, df_temp in evaluator.all_post_inters.items(): #NOTE only use trian post inter only\n",
    "    #print(u)\n",
    "    df_temp = df_temp.loc[df_temp.weight==1,['NoteID','Weeknum']]    \n",
    "    lst.append(df_temp.set_index('NoteID').to_dict()['Weeknum'])\n",
    "    ct+=len(df_temp)\n",
    "for dic in lst:\n",
    "    for k,v in dic.items():\n",
    "        post_create_dates_all[k] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pfilter = PostFilter()\n",
    "pfilter.fit(evaluator.all_note_contents)\n",
    "pfilter.prune_on_length(3)\n",
    "pfilter.prune_on_awl(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cleaned_data/db%d/pfilter.pickle'%dbnum, 'wb') as handle:\n",
    "    pickle.dump(pfilter, handle)\n",
    "with open('cleaned_data/db%d/post_create_dates_all.pickle'%dbnum, 'wb') as handle:\n",
    "    pickle.dump(post_create_dates_all, handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keywords(all_note_contents):\n",
    "    data_words, note_ids, id2word, corpus = utils.extract_vocab(all_note_contents, \n",
    "                                                                5, ['NOUN','VERB'], STOP_WORDS)\n",
    "    tfidf_matrix, tf_dicts, post_appear_dict = utils.tfidf(data_words)\n",
    "    keywords = {i: utils.get_top_tfidfs(tfidf_matrix[i], len(tfidf_matrix[i]) // 5) \n",
    "                for i, m in enumerate(tfidf_matrix)}\n",
    "    keywords\n",
    "    return note_ids, keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "note_ids, keywords = get_keywords(preprocessor.all_note_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = {note_ids[i]:m for i, m in keywords.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cleaned_data/db%d/precomputed_keyword_lookups.pickle'%dbnum, 'wb') as handle:\n",
    "    pickle.dump(keywords, handle)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
