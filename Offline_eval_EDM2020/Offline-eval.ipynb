{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from OfflineEvaluator import OfflineEvaluator\n",
    "from Preprocessor import Preprocessor\n",
    "import utils\n",
    "from STOPWORDS import STOP_WORDS\n",
    "\n",
    "import pickle\n",
    "import logging\n",
    "import operator\n",
    "import sys\n",
    "import os\n",
    "from collections import Counter\n",
    "import itertools\n",
    "from itertools import combinations\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import spacy\n",
    "import re\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample course SA\n",
    "dbnum = ##\n",
    "num_weeks = 6\n",
    "START_DATE = #datetime(...)\n",
    "TEMPORAL_START_WEEK = 3\n",
    "instructors = #[...]\n",
    "\n",
    "week_stride = 1\n",
    "ratio_read = 0.0\n",
    "k = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_clean = \"cleaned_data\"\n",
    "path_sample = \"sample\"\n",
    "# preprocess\n",
    "WEIGHT_POST_LOOKUP = {1: 'created',2:'liked',3:'linked',4:'replied',5:'revisited',6:'read',7:'anonymously read'}\n",
    "WEIGHT_USER_LOOKUP = {1:'liked',2:'linked',3:'replied',4:'read',5:'anonymously read'}\n",
    "CUTOFF_WEIGHT = 5\n",
    "EXPLICIT_THRESHOLD = 5\n",
    "IMPLICIT_THRESHOLD = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('cleaned_data/db%d'%dbnum)\n",
    "PRE_SIMS = {}\n",
    "with open('precomputed_item_similarities_use3.pickle', 'rb') as handle:\n",
    "    PRE_SIMS = pickle.load(handle)\n",
    "\n",
    "PRE_EMBS = {}\n",
    "with open('precomputed_sen_embs.pickle', 'rb') as handle:\n",
    "    PRE_EMBS = pickle.load(handle)\n",
    "\n",
    "PRE_KWS = {}\n",
    "with open('precomputed_keyword_lookups.pickle', 'rb') as handle:\n",
    "    PRE_KWS = pickle.load(handle)\n",
    "    \n",
    "with open('pfilter.pickle', 'rb') as handle:\n",
    "    pfilter = pickle.load(handle)\n",
    "\n",
    "with open('post_create_dates_all.pickle', 'rb') as handle:\n",
    "    post_create_dates_all = pickle.load(handle)\n",
    "    \n",
    "g = nx.read_graphml(\"content_graph-10.graphml\")\n",
    "mapping = {}\n",
    "for u,d in g.nodes(data=True):\n",
    "    if d['node_type']=='post':\n",
    "        mapping[u] = int(u)\n",
    "content_graph_1 = nx.relabel_nodes(g, mapping)\n",
    "\n",
    "g = nx.read_graphml(\"content_graph-0.2.graphml\")\n",
    "mapping = {}\n",
    "for u,d in g.nodes(data=True):\n",
    "    if d['node_type']=='post':\n",
    "        mapping[u] = int(u)\n",
    "content_graph_2 = nx.relabel_nodes(g, mapping)\n",
    "\n",
    "g = nx.read_graphml(\"content_graph-5.graphml\")\n",
    "mapping = {}\n",
    "for u,d in g.nodes(data=True):\n",
    "    if d['node_type']=='post':\n",
    "        mapping[u] = int(u)\n",
    "content_graph_3 = nx.relabel_nodes(g, mapping)\n",
    "os.chdir('../..')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune KCB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "''' Iterative Evaluation'''\n",
    "kcb_results = {}\n",
    "kcb_best_feature = {1:(15,3)}\n",
    "\n",
    "for cutoff_week in range(2, num_weeks):\n",
    "    print('Current Week: ', cutoff_week)\n",
    "    logging.info('........................... Offline Evaluator Starts..................................')\n",
    "    evaluator = OfflineEvaluator(WEIGHT_POST_LOOKUP, WEIGHT_USER_LOOKUP, CUTOFF_WEIGHT, \n",
    "                                 EXPLICIT_THRESHOLD, IMPLICIT_THRESHOLD, START_DATE, verbose=0)\n",
    "    evaluator.set_ground_truth_criterion('explicit_implicit')\n",
    "    evaluator.preprocess(path_sample, dbnum, cutoff_week, week_stride)\n",
    "    evaluator.fit(PRE_SIMS, PRE_EMBS, PRE_KWS, content_graph_2, instructors)\n",
    "    \n",
    "    '''\n",
    "    Calculate post creation dates\n",
    "    '''\n",
    "    lst = []\n",
    "    post_create_dates_all = {}\n",
    "    ct = 0\n",
    "    for u, df_temp in evaluator.train_post_inter.items(): #NOTE only use trian post inter only\n",
    "        #print(u)\n",
    "        df_temp = df_temp.loc[df_temp.weight==1,['NoteID','Weeknum']]    \n",
    "        lst.append(df_temp.set_index('NoteID').to_dict()['Weeknum'])\n",
    "        ct+=len(df_temp)\n",
    "    for dic in lst:\n",
    "        for key,v in dic.items():\n",
    "            post_create_dates_all[key] = v\n",
    "\n",
    "\n",
    "    #kcb_settings = ['tfidf+lsi', 'tokens_phrases', 15, 'averaging','explicit_implicit','cosine']\n",
    "    kcb_results_cur_wk = {}\n",
    "    for feature_size in [10,15,20]:\n",
    "        for top in [3,5]:\n",
    "            print('feature size',feature_size)\n",
    "            print('top ratio',top)\n",
    "            kcb_settings = ['KCB', 'tokens_phrases', feature_size, 'averaging','explicit_implicit','cosine', top]\n",
    "            recs = evaluator.run_cbf_eval(k, kcb_settings)\n",
    "            kcb_results_cur_wk[(feature_size, top)] = np.mean([v[1] for v in recs.values()])\n",
    "            if (feature_size, top) == kcb_best_feature.get(cutoff_week-1):\n",
    "                print(\"Used last week's best hyperparameters\")\n",
    "                kcb_results[cutoff_week] = [recs] # store the results based on last week's cross validation\n",
    "                \n",
    "    # get the best hyp selection from the current week, will be used for the next week\n",
    "    kcb_best_feature[cutoff_week] = sorted(kcb_results_cur_wk.items(), key=lambda kv: kv[1])[::-1][:1][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('results/db%d/%d-kcb-eval1_cv.pickle'%(dbnum,dbnum),'wb') as handle:\n",
    "    pickle.dump(kcb_results, handle)\n",
    "with open('results/db%d/%d-kcb-eval1_cv_best.pickle'%(dbnum, dbnum),'wb') as handle:\n",
    "    pickle.dump(kcb_best_feature, handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune MCF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "wmf_results = {}\n",
    "wmf_best_feature = {1:15}\n",
    "\n",
    "for cutoff_week in range(2, num_weeks):\n",
    "    logging.info('........................... Offline Evaluator Starts..................................')\n",
    "    evaluator = OfflineEvaluator(WEIGHT_POST_LOOKUP, WEIGHT_USER_LOOKUP, CUTOFF_WEIGHT, \n",
    "                                 EXPLICIT_THRESHOLD, IMPLICIT_THRESHOLD, START_DATE, verbose=0)\n",
    "    evaluator.set_ground_truth_criterion('explicit_implicit')\n",
    "    evaluator.preprocess(path_sample, dbnum, cutoff_week, week_stride)\n",
    "    evaluator.fit(PRE_SIMS, PRE_EMBS, PRE_KWS, content_graph_2, instructors)\n",
    "    \n",
    "    '''\n",
    "    Calculate post creation dates\n",
    "    '''\n",
    "    lst = []\n",
    "    post_create_dates_all = {}\n",
    "    ct = 0\n",
    "    for u, df_temp in evaluator.train_post_inter.items(): #NOTE only use trian post inter only\n",
    "        #print(u)\n",
    "        df_temp = df_temp.loc[df_temp.weight==1,['NoteID','Weeknum']]    \n",
    "        lst.append(df_temp.set_index('NoteID').to_dict()['Weeknum'])\n",
    "        ct+=len(df_temp)\n",
    "    for dic in lst:\n",
    "        for key,v in dic.items():\n",
    "            post_create_dates_all[key] = v\n",
    "        \n",
    "    wmf_results_cur_wk = {}\n",
    "    for alpha in [15, 20, 25, 30]:\n",
    "        wmf_settings = [alpha] # mode, #neighbors\n",
    "        recs = evaluator.run_als_eval(k, wmf_settings)\n",
    "        wmf_results_cur_wk[alpha] = np.mean([v[1] for v in recs.values()])\n",
    "        if alpha == wmf_best_feature.get(cutoff_week-1):\n",
    "            print(\"Used last week's best hyperparameters\")\n",
    "            wmf_results[cutoff_week] = [recs] # store the results based on last week's cross validation        \n",
    "    # get the best hyp selection from the current week, will be used for the next week\n",
    "    wmf_best_feature[cutoff_week] = sorted(wmf_results_cur_wk.items(), key=lambda kv: kv[1])[::-1][:1][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('results/db%d'%dbnum)\n",
    "with open('%d-wmf-eval1_cv.pickle'%(dbnum),'wb') as handle:\n",
    "    pickle.dump(wmf_results, handle)\n",
    "with open('%d-wmf-eval1_cv_best.pickle'%(dbnum),'wb') as handle:\n",
    "    pickle.dump(wmf_best_feature, handle)\n",
    "os.chdir('../..')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune PPR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pure_ppr_results = {}\n",
    "pure_ppr_best_feature = {1:0.85}\n",
    "\n",
    "for cutoff_week in range(2, num_weeks):\n",
    "    logging.info('........................... Offline Evaluator Starts..................................')\n",
    "    evaluator = OfflineEvaluator(WEIGHT_POST_LOOKUP, WEIGHT_USER_LOOKUP, CUTOFF_WEIGHT, \n",
    "                                 EXPLICIT_THRESHOLD, IMPLICIT_THRESHOLD, START_DATE, verbose=0)\n",
    "    evaluator.set_ground_truth_criterion('explicit_implicit')\n",
    "    evaluator.preprocess(path_sample, dbnum, cutoff_week, week_stride)\n",
    "    evaluator.fit(PRE_SIMS, PRE_EMBS, PRE_KWS, content_graph_2, instructors)\n",
    "    \n",
    "    '''\n",
    "    Calculate post creation dates\n",
    "    '''\n",
    "    lst = []\n",
    "    post_create_dates_all = {}\n",
    "    ct = 0\n",
    "    for u, df_temp in evaluator.train_post_inter.items(): #NOTE only use trian post inter only\n",
    "        #print(u)\n",
    "        df_temp = df_temp.loc[df_temp.weight==1,['NoteID','Weeknum']]    \n",
    "        lst.append(df_temp.set_index('NoteID').to_dict()['Weeknum'])\n",
    "        ct+=len(df_temp)\n",
    "    for dic in lst:\n",
    "        for key,v in dic.items():\n",
    "            post_create_dates_all[key] = v\n",
    "        \n",
    "    pure_ppr_results_cur_wk = {}\n",
    "    for damp in [0.75, 0.8, 0.85]:\n",
    "        print('damp',damp)\n",
    "        recs = evaluator.run_pure_ppr_eval(k, damp)\n",
    "        pure_ppr_results_cur_wk[damp] = np.mean([v[1] for v in recs.values()])\n",
    "        if damp == pure_ppr_best_feature.get(cutoff_week-1):\n",
    "            print(\"Used last week's best hyperparameters\")\n",
    "            pure_ppr_results[cutoff_week] = [recs] # store the results based on last week's cross validation        \n",
    "    # get the best hyp selection from the current week, will be used for the next week\n",
    "    pure_ppr_best_feature[cutoff_week] = sorted(pure_ppr_results_cur_wk.items(), \n",
    "                                                key=lambda kv: kv[1])[::-1][:1][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('results/db%d'%dbnum)\n",
    "with open('%d-pure_ppr-eval1_cv.pickle'%(dbnum),'wb') as handle:\n",
    "    pickle.dump(pure_ppr_results, handle)\n",
    "with open('%d-pure_ppr-eval1_cv_best.pickle'%(dbnum),'wb') as handle:\n",
    "    pickle.dump(pure_ppr_best_feature, handle)\n",
    "os.chdir('../..')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune CSCLRec, CoPPR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "csclrec_results = {}\n",
    "coppr_results = {}\n",
    "csclrec_best_feature = {1:(0.75,7,0.9)}\n",
    "coppr_best_feature = {1:(0.75,7,0.9)}\n",
    "\n",
    "for cutoff_week in range(2, num_weeks):\n",
    "    logging.info('........................... Offline Evaluator Starts..................................')\n",
    "    evaluator = OfflineEvaluator(WEIGHT_POST_LOOKUP, WEIGHT_USER_LOOKUP, CUTOFF_WEIGHT, \n",
    "                                 EXPLICIT_THRESHOLD, IMPLICIT_THRESHOLD, START_DATE, verbose=0)\n",
    "    evaluator.set_ground_truth_criterion('explicit_implicit')\n",
    "    evaluator.preprocess(path_sample, dbnum, cutoff_week, week_stride)\n",
    "    evaluator.fit(PRE_SIMS, PRE_EMBS, PRE_KWS, content_graph_2, instructors)\n",
    "    \n",
    "    '''\n",
    "    Calculate post creation dates\n",
    "    '''\n",
    "    lst = []\n",
    "    post_create_dates_all = {}\n",
    "    ct = 0\n",
    "    for u, df_temp in evaluator.train_post_inter.items(): #NOTE only use trian post inter only\n",
    "        df_temp = df_temp.loc[df_temp.weight==1,['NoteID','Weeknum']]    \n",
    "        lst.append(df_temp.set_index('NoteID').to_dict()['Weeknum'])\n",
    "        ct+=len(df_temp)\n",
    "    for dic in lst:\n",
    "        for key,v in dic.items():\n",
    "            post_create_dates_all[key] = v\n",
    "    \n",
    "    csclrec_results_cur_wk = {}\n",
    "    for temporal in [0.7,0.75,0.8]:\n",
    "        for nn in [3,5,7,10]:\n",
    "            for damp in [0.8,0.85,0.9]:\n",
    "                print('temporal decay',temporal)\n",
    "                print('nn',nn)\n",
    "                print('damping factor',damp)\n",
    "                csclrec_settings = {'sna':True, 'cbf':True, 'hybrid':[0.1, 1.0, 0.01],\n",
    "                                 'temporal_decay':temporal, '#neighbours': nn,'damp':damp}\n",
    "                recs, ups = evaluator.run_csclrec_eval(k, csclrec_settings, pfilter, \n",
    "                                                   post_create_dates_all, \n",
    "                                                   TEMPORAL_START_WEEK, \n",
    "                                                   ratio_read)\n",
    "                csclrec_results_cur_wk[(temporal, nn, damp)] = np.mean([v[1] for v in recs.values()])\n",
    "                if (temporal, nn, damp) == csclrec_best_feature.get(cutoff_week-1):\n",
    "                    print(\"Used last week's best hyperparameters\")\n",
    "                    csclrec_results[cutoff_week] = [recs, ups] # store the results based on last week's cross validation\n",
    "\n",
    "    # get the best hyp selection from the current week, will be used for the next week\n",
    "    csclrec_best_feature[cutoff_week] = sorted(csclrec_results_cur_wk.items(), key=lambda kv: kv[1])[::-1][:1][0][0]\n",
    "    \n",
    "    coppr_results_cur_wk = {}\n",
    "    for temporal in [0.7,0.75,0.8]:\n",
    "        for nn in [3,5,7,10]:\n",
    "            for damp in [0.8,0.85,0.9]:\n",
    "                print('temporal decay',temporal)\n",
    "                print('nn',nn)\n",
    "                print('damping factor',damp)\n",
    "                coppr_settings = {'sna':True, 'cbf':True, 'hybrid':[0.1, 1.0, 1.0, 1.0],\n",
    "                                 'temporal_decay':temporal, '#neighbours': nn,'damp':damp}\n",
    "                recs, ups = evaluator.run_coppr_eval(k, coppr_settings, pfilter, \n",
    "                                                   post_create_dates_all, \n",
    "                                                   TEMPORAL_START_WEEK, \n",
    "                                                   ratio_read)\n",
    "                coppr_results_cur_wk[(temporal, nn, damp)] = np.mean([v[1] for v in recs.values()])\n",
    "                if (temporal, nn, damp) == coppr_best_feature.get(cutoff_week-1):\n",
    "                    print(\"Used last week's best hyperparameters\")\n",
    "                    coppr_results[cutoff_week] = [recs, ups] # store the results based on last week's cross validation\n",
    "\n",
    "    # get the best hyp selection from the current week, will be used for the next week\n",
    "    coppr_best_feature[cutoff_week] = sorted(coppr_results_cur_wk.items(), key=lambda kv: kv[1])[::-1][:1][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('results/db%d/%d-csclrec-eval1_cv.pickle'%(dbnum,dbnum),'wb') as handle:\n",
    "    pickle.dump(csclrec_results, handle)\n",
    "with open('results/db%d/%d-csclrec-eval1_cv_best.pickle'%(dbnum,dbnum),'wb') as handle:\n",
    "    pickle.dump(csclrec_best_feature, handle)\n",
    "with open('results/db%d/%d-coppr-eval1_cv.pickle'%(dbnum,dbnum),'wb') as handle:\n",
    "    pickle.dump(coppr_results, handle)\n",
    "with open('results/db%d/%d-coppr-eval1_cv_best.pickle'%(dbnum, dbnum),'wb') as handle:\n",
    "    pickle.dump(coppr_best_feature, handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RND, PPL, and SCB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "''' Iterative Evaluation'''\n",
    "\n",
    "scb_results = {}\n",
    "random_results = {}\n",
    "ppl_results = {}\n",
    "\n",
    "week_stride = 1\n",
    "ratio_read = 0.0\n",
    "k = 10    \n",
    "\n",
    "for cutoff_week in range(2, num_weeks):\n",
    "    logging.info('........................... Offline Evaluator Starts..................................')\n",
    "    evaluator = OfflineEvaluator(WEIGHT_POST_LOOKUP, WEIGHT_USER_LOOKUP, CUTOFF_WEIGHT, \n",
    "                                 EXPLICIT_THRESHOLD, IMPLICIT_THRESHOLD, START_DATE, verbose=0)\n",
    "    evaluator.set_ground_truth_criterion('explicit_implicit')\n",
    "    evaluator.preprocess(path_sample, dbnum, cutoff_week, week_stride)\n",
    "    evaluator.fit(PRE_SIMS, PRE_EMBS, PRE_KWS, content_graph_2, instructors)\n",
    "    \n",
    "    '''\n",
    "    Calculate post creation dates\n",
    "    '''\n",
    "    lst = []\n",
    "    post_create_dates_all = {}\n",
    "    ct = 0\n",
    "    for u, df_temp in evaluator.train_post_inter.items(): #NOTE only use trian post inter only\n",
    "        #print(u)\n",
    "        df_temp = df_temp.loc[df_temp.weight==1,['NoteID','Weeknum']]    \n",
    "        lst.append(df_temp.set_index('NoteID').to_dict()['Weeknum'])\n",
    "        ct+=len(df_temp)\n",
    "    for dic in lst:\n",
    "        for key,v in dic.items():\n",
    "            post_create_dates_all[key] = v\n",
    "\n",
    "\n",
    "    recs =  evaluator.run_random_eval(k)\n",
    "    random_results[cutoff_week] = [recs]\n",
    "            \n",
    "    recs =  evaluator.run_popularity_eval(k)\n",
    "    ppl_results[cutoff_week] = [recs]\n",
    "    \n",
    "    scb_settings = ['sentence_emb_precomputed', PRE_EMBS, PRE_SIMS, 'averaging','explicit_implicit','cosine']\n",
    "    recs = evaluator.run_cbf_eval(k, scb_settings)\n",
    "    scb_results[cutoff_week] = [recs]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "method_lookup = {0:'RND', 1:'PPL', 2:'SCB'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('results/db%d'%dbnum)\n",
    "for i, results in enumerate([random_results, ppl_results,scb_results]):\n",
    "    with open('%d-%s-results_eval1.pickle'%(dbnum,method_lookup[i]),'wb') as handle:\n",
    "        pickle.dump(results, handle)\n",
    "os.chdir('../..')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
